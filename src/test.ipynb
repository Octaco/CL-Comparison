{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from info_nce import InfoNCE"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T01:14:44.130055Z",
     "start_time": "2024-02-01T01:14:44.115056700Z"
    }
   },
   "id": "4183bd77e23eb864"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from transformers import RobertaModel, RobertaConfig, RobertaTokenizer\n",
    "\n",
    "\n",
    "class CustomDataset(TensorDataset):\n",
    "\n",
    "    def __init__(self, dataframe):\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "        self.doc = dataframe.doc\n",
    "        self.code = dataframe.code\n",
    "        # self.targets = dataframe.labels\n",
    "        self.max_len = 512\n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(self.doc) == len(self.code)\n",
    "        return len(self.doc)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        doc = str(self.doc[index])\n",
    "        doc = \" \".join(doc.split())\n",
    "        \n",
    "        code = str(self.code[index])\n",
    "        doc_inputs = self.tokenizer.encode_plus(\n",
    "            doc,\n",
    "            add_special_tokens=False,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=False\n",
    "        )\n",
    "        doc_ids = doc_inputs['input_ids']\n",
    "        doc_mask = doc_inputs['attention_mask']\n",
    "        \n",
    "        code_inputs = self.tokenizer.encode_plus(\n",
    "            code,\n",
    "            add_special_tokens=False,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=False\n",
    "        )\n",
    "        \n",
    "        code_ids = code_inputs['input_ids']\n",
    "        code_mask = code_inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'doc_ids': torch.tensor(doc_ids, dtype=torch.long),\n",
    "            'doc_mask': torch.tensor(doc_mask, dtype=torch.long),\n",
    "            'code_ids': torch.tensor(code_ids, dtype=torch.long),\n",
    "            'code_mask': torch.tensor(code_mask, dtype=torch.long),\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T00:58:10.579669200Z",
     "start_time": "2024-02-01T00:58:10.573671100Z"
    }
   },
   "id": "3d4f763465494c1f"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "RobertaModel(\n  (embeddings): RobertaEmbeddings(\n    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n    (position_embeddings): Embedding(514, 768, padding_idx=1)\n    (token_type_embeddings): Embedding(1, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): RobertaEncoder(\n    (layer): ModuleList(\n      (0-11): 12 x RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): RobertaPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RobertaModel.from_pretrained('microsoft/codebert-base')\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-5)\n",
    "model.to(torch.device(\"cuda\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T00:58:13.526252500Z",
     "start_time": "2024-02-01T00:58:12.042246800Z"
    }
   },
   "id": "df6db932e163e2df"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "train_params = {'batch_size': 7,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': 7,\n",
    "               'shuffle': False,\n",
    "               'num_workers': 0\n",
    "               }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T00:58:13.546254100Z",
     "start_time": "2024-02-01T00:58:13.528255400Z"
    }
   },
   "id": "78dc0cfc43bbb57e"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "code_search_dataset = load_dataset('code_search_net', 'ruby')\n",
    "\n",
    "# train_data\n",
    "train_data = code_search_dataset['train']\n",
    "\n",
    "function_code = train_data['func_code_string']\n",
    "function_documentation = train_data['func_documentation_string']\n",
    "\n",
    "train_df =pd.DataFrame()\n",
    "train_df['doc'] = function_documentation\n",
    "train_df['code'] = function_code\n",
    "\n",
    "# test_data\n",
    "test_data = code_search_dataset['test']\n",
    "\n",
    "function_code_test = test_data['func_code_string']\n",
    "function_documentation_test = test_data['func_documentation_string']\n",
    "\n",
    "test_df =pd.DataFrame()\n",
    "test_df['doc'] = function_documentation_test\n",
    "test_df['code'] = function_code_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T00:58:17.751294100Z",
     "start_time": "2024-02-01T00:58:16.215265100Z"
    }
   },
   "id": "f1dfd02b3a488259"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "train_dataset = train_df.sample(frac=train_size, random_state=200)\n",
    "valid_dataset = train_df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "test_dataset = test_df.reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T00:58:40.145357900Z",
     "start_time": "2024-02-01T00:58:40.121354200Z"
    }
   },
   "id": "8cd29470db98b2bf"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Dataset: (39033, 2)\n",
      "VAL Dataset: (9758, 2)\n",
      "TEST Dataset: (2279, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"VAL Dataset: {}\".format(valid_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T00:58:40.633887Z",
     "start_time": "2024-02-01T00:58:40.601889200Z"
    }
   },
   "id": "e72f5da8ee427eae"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "training_set = CustomDataset(train_dataset)\n",
    "loss_formulation = InfoNCE(negative_mode='unpaired')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T01:14:48.615652500Z",
     "start_time": "2024-02-01T01:14:48.334655400Z"
    }
   },
   "id": "f382bffe184fdfa"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[52], line 25\u001B[0m\n\u001B[0;32m     23\u001B[0m query_mask \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdoc_mask\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m     24\u001B[0m inputs \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m'\u001B[39m: query_id, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m'\u001B[39m: query_mask}\n\u001B[1;32m---> 25\u001B[0m query \u001B[38;5;241m=\u001B[39m model(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs)[\u001B[38;5;241m1\u001B[39m]  \u001B[38;5;66;03m# using pooled values\u001B[39;00m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;66;03m#keys = code\u001B[39;00m\n\u001B[0;32m     27\u001B[0m code_list \u001B[38;5;241m=\u001B[39m [(batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcode_ids\u001B[39m\u001B[38;5;124m'\u001B[39m][i]\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m)), batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcode_mask\u001B[39m\u001B[38;5;124m'\u001B[39m][i]\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m))) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcode_ids\u001B[39m\u001B[38;5;124m'\u001B[39m]))]\n",
      "File \u001B[1;32m~\\PycharmProjects\\CL-Comparison\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\PycharmProjects\\CL-Comparison\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\CL-Comparison\\venv\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:789\u001B[0m, in \u001B[0;36mRobertaModel.forward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    786\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    787\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou have to specify either input_ids or inputs_embeds\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 789\u001B[0m batch_size, seq_length \u001B[38;5;241m=\u001B[39m input_shape\n\u001B[0;32m    790\u001B[0m device \u001B[38;5;241m=\u001B[39m input_ids\u001B[38;5;241m.\u001B[39mdevice \u001B[38;5;28;01mif\u001B[39;00m input_ids \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m inputs_embeds\u001B[38;5;241m.\u001B[39mdevice\n\u001B[0;32m    792\u001B[0m \u001B[38;5;66;03m# past_key_values_length\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 1+1):\n",
    "    #model.train()\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    #shuffle the data\n",
    "    #random.shuffle(training_set)\n",
    "    batch_size = train_params['batch_size']\n",
    "    train_dataloader = DataLoader(training_set, batch_size=7, shuffle=True)\n",
    "    \n",
    "\n",
    "    \n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        if idx > 2:\n",
    "            break\n",
    "        \n",
    "        if len(batch) <= 1:\n",
    "            continue\n",
    "        \n",
    "        # query = doc\n",
    "        query_id = batch['doc_ids'][0].to(torch.device(\"cuda\"))\n",
    "        query_mask = batch['doc_mask'][0].to(torch.device(\"cuda\"))\n",
    "        inputs = {'input_ids': query_id, 'attention_mask': query_mask}\n",
    "        query = model(**inputs)[1]  # using pooled values\n",
    "        #keys = code\n",
    "        code_list = [(batch['code_ids'][i].to(torch.device(\"cuda\")), batch['code_mask'][i].to(torch.device(\"cuda\"))) for i in range(len(batch['code_ids']))]\n",
    "        \n",
    "        positive_code_key = code_list.pop(0)\n",
    "        inputs = {'input_ids': positive_code_key[0], 'attention_mask': positive_code_key[1]}\n",
    "        positive_code_key = model(**inputs)[1] # using pooled values\n",
    "        \n",
    "        \n",
    "        for code in code_list:\n",
    "            inputs = {'input_ids': code[0], 'attention_mask': code[1]}\n",
    "            code = model(**inputs)[1] # using pooled values\n",
    "            code = code.unsqueeze(0)\n",
    "            code_list.append(code)\n",
    "    \n",
    "        \n",
    "        \n",
    "        loss = loss_formulation(query, positive_code_key, code_list)\n",
    "        print(loss)\n",
    "        \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T01:27:25.833953500Z",
     "start_time": "2024-02-01T01:27:25.757951200Z"
    }
   },
   "id": "205d3a8f3ffc7e9c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test2 = training_set[2:3]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc723eeb81978efe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6a5d113331ac8eeb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
