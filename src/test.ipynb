{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from info_nce import InfoNCE"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T01:40:42.890474900Z",
     "start_time": "2024-02-06T01:40:38.560477500Z"
    }
   },
   "id": "4183bd77e23eb864"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from transformers import RobertaModel, RobertaConfig, RobertaTokenizer\n",
    "\n",
    "\n",
    "class CustomDataset(TensorDataset):\n",
    "\n",
    "    def __init__(self, dataframe):\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "        self.doc = dataframe.doc\n",
    "        self.code = dataframe.code\n",
    "        # self.targets = dataframe.labels\n",
    "        self.max_len = 512\n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(self.doc) == len(self.code)\n",
    "        return len(self.doc)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        doc = str(self.doc[index])\n",
    "        doc = \" \".join(doc.split())\n",
    "        \n",
    "        code = str(self.code[index])\n",
    "        doc_inputs = self.tokenizer.encode_plus(\n",
    "            doc,\n",
    "            add_special_tokens=False,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=False\n",
    "        )\n",
    "        doc_ids = doc_inputs['input_ids']\n",
    "        doc_mask = doc_inputs['attention_mask']\n",
    "        \n",
    "        code_inputs = self.tokenizer.encode_plus(\n",
    "            code,\n",
    "            add_special_tokens=False,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=False\n",
    "        )\n",
    "        \n",
    "        code_ids = code_inputs['input_ids']\n",
    "        code_mask = code_inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'doc_ids': torch.tensor(doc_ids, dtype=torch.long),\n",
    "            'doc_mask': torch.tensor(doc_mask, dtype=torch.long),\n",
    "            'code_ids': torch.tensor(code_ids, dtype=torch.long),\n",
    "            'code_mask': torch.tensor(code_mask, dtype=torch.long),\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T01:40:43.413477200Z",
     "start_time": "2024-02-06T01:40:42.861476300Z"
    }
   },
   "id": "3d4f763465494c1f"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "RobertaModel(\n  (embeddings): RobertaEmbeddings(\n    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n    (position_embeddings): Embedding(514, 768, padding_idx=1)\n    (token_type_embeddings): Embedding(1, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): RobertaEncoder(\n    (layer): ModuleList(\n      (0-11): 12 x RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): RobertaPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RobertaModel.from_pretrained('microsoft/codebert-base')\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-5)\n",
    "model.to(torch.device(\"cuda\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T01:40:45.691489300Z",
     "start_time": "2024-02-06T01:40:43.414474500Z"
    }
   },
   "id": "df6db932e163e2df"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "train_params = {'batch_size': 7,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': 7,\n",
    "               'shuffle': False,\n",
    "               'num_workers': 0\n",
    "               }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T01:40:45.692488Z",
     "start_time": "2024-02-06T01:40:45.667489900Z"
    }
   },
   "id": "78dc0cfc43bbb57e"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "code_search_dataset = load_dataset('code_search_net', 'ruby')\n",
    "\n",
    "# train_data\n",
    "train_data = code_search_dataset['train']\n",
    "\n",
    "function_code = train_data['func_code_string']\n",
    "function_documentation = train_data['func_documentation_string']\n",
    "\n",
    "train_df =pd.DataFrame()\n",
    "train_df['doc'] = function_documentation\n",
    "train_df['code'] = function_code\n",
    "\n",
    "# test_data\n",
    "test_data = code_search_dataset['test']\n",
    "\n",
    "function_code_test = test_data['func_code_string']\n",
    "function_documentation_test = test_data['func_documentation_string']\n",
    "\n",
    "test_df =pd.DataFrame()\n",
    "test_df['doc'] = function_documentation_test\n",
    "test_df['code'] = function_code_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T01:40:47.518490200Z",
     "start_time": "2024-02-06T01:40:45.681487800Z"
    }
   },
   "id": "f1dfd02b3a488259"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "train_dataset = train_df.sample(frac=train_size, random_state=200)\n",
    "valid_dataset = train_df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "test_dataset = test_df.reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T01:40:47.536488500Z",
     "start_time": "2024-02-06T01:40:47.506489Z"
    }
   },
   "id": "8cd29470db98b2bf"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Dataset: (39033, 2)\n",
      "VAL Dataset: (9758, 2)\n",
      "TEST Dataset: (2279, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"VAL Dataset: {}\".format(valid_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T01:40:47.580488200Z",
     "start_time": "2024-02-06T01:40:47.538488700Z"
    }
   },
   "id": "e72f5da8ee427eae"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "training_set = CustomDataset(train_dataset)\n",
    "loss_formulation = InfoNCE(negative_mode='unpaired')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T01:40:47.771487800Z",
     "start_time": "2024-02-06T01:40:47.551490900Z"
    }
   },
   "id": "f382bffe184fdfa"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9226, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8411, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7655, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 1+1):\n",
    "    #model.train()\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    #shuffle the data\n",
    "    #random.shuffle(training_set)\n",
    "    batch_size = train_params['batch_size']\n",
    "    train_dataloader = DataLoader(training_set, batch_size=7, shuffle=True)\n",
    "    \n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        if idx > 2:\n",
    "            break\n",
    "        \n",
    "        if len(batch) <= 1:\n",
    "            continue\n",
    "        \n",
    "        # query = doc\n",
    "        query_id = batch['doc_ids'][0].to(torch.device(\"cuda\")).unsqueeze(0)\n",
    "        query_mask = batch['doc_mask'][0].to(torch.device(\"cuda\")).unsqueeze(0)\n",
    "        inputs = {'input_ids': query_id, 'attention_mask': query_mask}\n",
    "        query = model(**inputs)[1]  # using pooled values\n",
    "        #keys = code\n",
    "        code_list = [(batch['code_ids'][i].unsqueeze(0).to(torch.device(\"cuda\")), batch['code_mask'][i].unsqueeze(0).to(torch.device(\"cuda\"))) for i in range(1, batch_size)]\n",
    "        \n",
    "        positive_code_key = code_list.pop(0)\n",
    "        inputs = {'input_ids': positive_code_key[0], 'attention_mask': positive_code_key[1]}\n",
    "        positive_code_key = model(**inputs)[1] # using pooled values\n",
    "        \n",
    "        negative_keys = []\n",
    "        \n",
    "        \n",
    "        for code, mask in code_list:\n",
    "            inputs = {'input_ids': code, 'attention_mask': mask}\n",
    "            negative_key = model(**inputs)[1] # using pooled values\n",
    "        \n",
    "            negative_keys.append(negative_key.clone().detach())\n",
    "    \n",
    "        negative_keys_reshaped = torch.cat(negative_keys, dim=0)    \n",
    "        \n",
    "        loss = loss_formulation(query, positive_code_key, negative_keys_reshaped)\n",
    "        loss.backward()\n",
    "        \n",
    "        losses.append(loss)\n",
    "        num_of_accumulation_steps = 10\n",
    "        \n",
    "        if (idx+1) % num_of_accumulation_steps == 0:\n",
    "            optimizer.zero_grad()\n",
    "            optimizer.step()\n",
    "        print(loss)\n",
    "        \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T02:13:04.693361400Z",
     "start_time": "2024-02-06T02:12:53.557579500Z"
    }
   },
   "id": "205d3a8f3ffc7e9c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test2 = training_set[2:3]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc723eeb81978efe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6a5d113331ac8eeb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
